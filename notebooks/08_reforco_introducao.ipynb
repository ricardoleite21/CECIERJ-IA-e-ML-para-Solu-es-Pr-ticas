{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9bbe7b",
   "metadata": {},
   "source": [
    "\n",
    "# Capítulo 8 — Introdução ao Aprendizado por Reforço\n",
    "**Conceitos:** agente, ambiente, política, recompensa, valor.  \n",
    "**Exemplo didático:** Q-Learning tabular em um grid 1D simples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, random\n",
    "\n",
    "n_states = 10\n",
    "terminal = n_states - 1\n",
    "actions = [0, 1]  # 0: esquerda, 1: direita\n",
    "Q = np.zeros((n_states, len(actions)))\n",
    "\n",
    "def step(s, a):\n",
    "    if a == 1: s2 = min(s+1, terminal)\n",
    "    else:      s2 = max(s-1, 0)\n",
    "    r = 1.0 if s2 == terminal else -0.01\n",
    "    done = (s2 == terminal)\n",
    "    return s2, r, done\n",
    "\n",
    "alpha, gamma, eps = 0.1, 0.95, 0.1\n",
    "episodes = 500\n",
    "for _ in range(episodes):\n",
    "    s, done = 0, False\n",
    "    while not done:\n",
    "        a = np.argmax(Q[s]) if random.random() > eps else random.choice(actions)\n",
    "        s2, r, done = step(s, a)\n",
    "        Q[s, a] += alpha*(r + gamma*np.max(Q[s2]) - Q[s, a])\n",
    "        s = s2\n",
    "\n",
    "# Teste da política aprendida\n",
    "s, path = 0, [0]\n",
    "while s != terminal and len(path) < 50:\n",
    "    a = np.argmax(Q[s])\n",
    "    s, _, _ = step(s, a)\n",
    "    path.append(s)\n",
    "print(\"Caminho aprendido:\", path)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
