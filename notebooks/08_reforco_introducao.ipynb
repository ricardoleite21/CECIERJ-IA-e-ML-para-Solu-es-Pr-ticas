{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf975861",
   "metadata": {},
   "source": [
    "\n",
    "# Capítulo 8 — Introdução ao Aprendizado por Reforço (Q-Learning tabular)\n",
    "\n",
    "**Curso:** CECIERJ – IA e ML para Soluções Práticas  \n",
    "**Objetivo:** entender conceitos de **agente, ambiente, estados, ações, recompensas** e implementar um exemplo prático simples de **Q-Learning tabular**.\n",
    "\n",
    "---\n",
    "## Conceitos básicos\n",
    "- **Agente**: quem aprende e toma decisões.  \n",
    "- **Ambiente**: onde o agente interage.  \n",
    "- **Estado (s)**: representação da situação atual.  \n",
    "- **Ações (a)**: escolhas possíveis do agente.  \n",
    "- **Recompensa (r)**: feedback recebido após ação.  \n",
    "- **Política (π)**: estratégia do agente (qual ação escolher).  \n",
    "- **Q-Learning**: algoritmo que aprende a função de valor de ação (Q(s,a)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d5d2ce",
   "metadata": {},
   "source": [
    "\n",
    "## Nosso ambiente didático\n",
    "- Linha de 0 até `N-1` (grid 1D).  \n",
    "- Estado inicial = 0.  \n",
    "- Objetivo = alcançar o estado final (N-1).  \n",
    "- Ações: esquerda (0) ou direita (1).  \n",
    "- Recompensa: +1 ao chegar no final, -0.01 a cada passo.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "N = 10   # número de estados\n",
    "terminal = N-1\n",
    "actions = [0,1]  # 0 = esquerda, 1 = direita\n",
    "\n",
    "def step(s, a):\n",
    "    if a == 1:  # direita\n",
    "        s2 = min(s+1, terminal)\n",
    "    else:       # esquerda\n",
    "        s2 = max(s-1, 0)\n",
    "    r = 1.0 if s2 == terminal else -0.01\n",
    "    done = (s2 == terminal)\n",
    "    return s2, r, done\n",
    "\n",
    "# Teste rápido do ambiente\n",
    "print(step(0,1))  # do estado 0, ação direita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b687c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "Q = np.zeros((N, len(actions)))\n",
    "\n",
    "alpha, gamma, eps = 0.1, 0.95, 0.1\n",
    "episodes = 500\n",
    "\n",
    "for ep in range(episodes):\n",
    "    s, done = 0, False\n",
    "    while not done:\n",
    "        if random.random() < eps:\n",
    "            a = random.choice(actions)\n",
    "        else:\n",
    "            a = np.argmax(Q[s])\n",
    "        s2, r, done = step(s, a)\n",
    "        Q[s,a] += alpha * (r + gamma*np.max(Q[s2]) - Q[s,a])\n",
    "        s = s2\n",
    "\n",
    "print(\"Q-table aprendida (primeiros estados):\")\n",
    "print(Q[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c56ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s, path = 0, [0]\n",
    "while s != terminal and len(path) < 50:\n",
    "    a = np.argmax(Q[s])\n",
    "    s, _, _ = step(s, a)\n",
    "    path.append(s)\n",
    "print(\"Caminho aprendido:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9934c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "save_path = \"/mnt/data/modelo_cap8_qtable.joblib\"\n",
    "joblib.dump(Q, save_path)\n",
    "print(\"Q-table salva em:\", save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c3c03",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Conclusões\n",
    "- O agente aprendeu a **ir para a direita** até alcançar o objetivo (estado final).  \n",
    "- Q-Learning ajusta valores Q(s,a) iterativamente com base em experiências.  \n",
    "- Conceitos mostrados aqui são base para problemas reais (ex.: jogos, logística, robótica).  \n",
    "- Em aplicações reais, estados/ações são muito maiores → usamos **aproximação por redes neurais (Deep RL)**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
